p8105_hw2_zz3166
================
Zihan Zhao
2024-10-02

### Problem 0

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
library(dplyr)
library(readr)
library(janitor)
```

    ## 
    ## Attaching package: 'janitor'
    ## 
    ## The following objects are masked from 'package:stats':
    ## 
    ##     chisq.test, fisher.test

### Problem 1

Data Import and Cleaning

``` r
trans_ent = 
  read_csv(
    "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

Dataset Description

The cleaned dataset contains information about subway station entrances
and exits in New York City. It includes the following variables:

- **line**: Subway line where the station is located.
- **station_name**: Name of the station.
- **station_latitude** and **station_longitude**: Geographic coordinates
  of the station.
- **route1** to **route11**: Subway routes that serve the station.
- **entry**: Logical indicator of whether the entrance allows entry
  (`TRUE`) or not (`FALSE`).
- **vending**: Indicates if there are vending machines (`YES`/`NO`).
- **entrance_type**: Type of the entrance (e.g., `STAIR`, `ELEVATOR`).
- **ada**: Logical indicator of ADA compliance (`TRUE` if compliant).

``` r
trans_ent |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

- **Answer**: There are **465** distinct stations in the street.

``` r
trans_ent |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

- **Answer**: There are **84** ADA compliant stations.

``` r
trans_ent |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

    ## [1] 0.3770492

- **Answer**: Approximately **0.377** (37.7%) of station entrances/exits
  without vending allow entry.

Stations serving the A train and ADA Compliance

``` r
trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # ℹ 50 more rows

- **Answer**:There are **60** distinct stations that serve the A train.

``` r
trans_ent |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

- **Answer**: There are **17** ADA compliant stations that serve the A
  train.

### Problem 2

Step 1: Import and Clean the Data

Import `Mr_Trashwheel` dataset.

``` r
Mr_Trashwheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(sports_balls = as.integer(round(sports_balls, 0)),
         source = "Mr. Trash Wheel")
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

Import `Professor_Trashwheel` dataset.

``` r
Professor_Trashwheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  select(dumpster, month, year, date, weight_tons , volume_cubic_yards,
         plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers, homes_powered) |> 
  mutate(year = as.character(year),  # Convert year to character for consistency
         source = "Professor Trash Wheel")
```

Import `Gwynnda_Trashwheel` dataset.

``` r
Gwynnda_Trashwheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  select(dumpster, month, year, date, weight_tons, volume_cubic_yards,
         plastic_bottles, polystyrene, cigarette_butts, plastic_bags, wrappers, homes_powered) |> 
  mutate(year = as.character(year),  # Convert year to character for consistency
         source = "Gwynnda")
```

Step 2: Combine the datasets.

``` r
Combined_Trashwheel = bind_rows(Mr_Trashwheel, Professor_Trashwheel, Gwynnda_Trashwheel)
glimpse(Combined_Trashwheel)
```

    ## Rows: 1,033
    ## Columns: 17
    ## $ dumpster           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …
    ## $ month              <chr> "May", "May", "May", "May", "May", "May", "May", "M…
    ## $ year               <chr> "2014", "2014", "2014", "2014", "2014", "2014", "20…
    ## $ date               <dttm> 2014-05-16, 2014-05-16, 2014-05-16, 2014-05-17, 20…
    ## $ weight_tons        <dbl> 4.31, 2.74, 3.45, 3.10, 4.06, 2.71, 1.91, 3.70, 2.5…
    ## $ volume_cubic_yards <dbl> 18, 13, 15, 15, 18, 13, 8, 16, 14, 18, 15, 19, 15, …
    ## $ plastic_bottles    <dbl> 1450, 1120, 2450, 2380, 980, 1430, 910, 3580, 2400,…
    ## $ polystyrene        <dbl> 1820, 1030, 3100, 2730, 870, 2140, 1090, 4310, 2790…
    ## $ cigarette_butts    <dbl> 126000, 91000, 105000, 100000, 120000, 90000, 56000…
    ## $ glass_bottles      <dbl> 72, 42, 50, 52, 72, 46, 32, 58, 49, 75, 38, 45, 58,…
    ## $ plastic_bags       <dbl> 584, 496, 1080, 896, 368, 672, 416, 1552, 984, 448,…
    ## $ wrappers           <dbl> 1162, 874, 2032, 1971, 753, 1144, 692, 3015, 1988, …
    ## $ sports_balls       <int> 7, 5, 6, 6, 7, 5, 3, 6, 6, 7, 6, 8, 6, 6, 6, 6, 5, …
    ## $ homes_powered      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
    ## $ x15                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
    ## $ x16                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
    ## $ source             <chr> "Mr. Trash Wheel", "Mr. Trash Wheel", "Mr. Trash Wh…

Step 3: Calculate Total weight of trash collected by Professor Trash
Wheel

``` r
total_weight_professor = Combined_Trashwheel |> 
  filter(source == "Professor Trash Wheel") |> 
  summarise(Total_Weight_Tons = sum(weight_tons, na.rm = TRUE))

total_weight_professor
```

    ## # A tibble: 1 × 1
    ##   Total_Weight_Tons
    ##               <dbl>
    ## 1              247.

Step 4: Calculate Total cigarette butts collected by Gwynnda in June
2022

``` r
total_cig_butts_gwynnda = Combined_Trashwheel |> 
  filter(source == "Gwynnda", year == "2022", month == "June") |> 
  summarise(Total_Cigarette_Butts = sum(cigarette_butts, na.rm = TRUE))
total_cig_butts_gwynnda
```

    ## # A tibble: 1 × 1
    ##   Total_Cigarette_Butts
    ##                   <dbl>
    ## 1                 18120

- **Summary**: “The combined dataset has **1033** observations. It
  includes key variables like total weight of trash (`weight_tons`),
  number of plastic bottles (`plastic_bottles`), and cigarette butts
  (`cigarette_butts`). The total weight of trash collected by Professor
  Trash Wheel is **246.74** tons. Gwynnda collected **18120** cigarette
  butts in June 2022.”

### Problem 3

This problem analyzes data on bakers, their bakes, and results from
Seasons 1 through 10 of the Great British Bake Off. The goal is to clean
and merge datasets from bakers, bakes, and results, and analyze key
outcomes such as “Star Baker” and viewership trends.

Step 1 : Data Import and Cleaning

We begin by importing and cleaning the `bakers.csv`, `bakes.csv`, and
`results.csv` files.

``` r
# Import and clean bakers dataset
bakers <- 
  read_csv("data/bakers.csv", na = c("NA", "", ".", "N/A")) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker", "baker_last_name"), sep = " ") |> 
  arrange(series)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

- **Description**: The `bakers.csv` file contains information about the
  bakers (e.g., name, age, occupation). We split the baker’s full name
  into first and last names for clarity.

``` r
# Import and clean bakes dataset
bakes <- 
  read_csv("data/bakes.csv", na = c("NA", "", ".", "N/A")) |> 
  janitor::clean_names() |> 
  mutate(baker = str_replace_all(baker, '"Jo"', "Jo"))
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

- **Description**: The `bakes.csv` file provides details about each
  baker’s performance in signature bakes and show stoppers. We fix a
  specific name inconsistency (`"Jo"`).

``` r
# Import and clean results dataset
results <- 
  read_csv("data/results.csv", na = c("NA", "", ".", "N/A"), skip = 2) |> 
  janitor::clean_names() |> 
  mutate(baker = if_else(baker == "Joanne", "Jo", baker))
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

- **Description**: The `results.csv` file contains the outcome of each
  episode, such as the technical challenge ranking and whether a baker
  was eliminated or named Star Baker. We address a naming issue where
  “Joanne” is renamed to “Jo” for consistency.

Step 2: Checking for Discrepancies

Before merging, we check for any bakers in the `bakes` and `results`
datasets that are missing from the `bakers` dataset.

``` r
# Check for discrepancies
missing_bakes <- anti_join(bakers, bakes)
```

    ## Joining with `by = join_by(baker, series)`

``` r
missing_results <- anti_join(bakers, results, by = "baker")

# Display any discrepancies
missing_bakes
```

    ## # A tibble: 25 × 6
    ##    baker   baker_last_name series baker_age baker_occupation            hometown
    ##    <chr>   <chr>            <dbl>     <dbl> <chr>                       <chr>   
    ##  1 Antony  Amourdoux            9        30 Banker                      London  
    ##  2 Briony  Williams             9        33 Full-time parent            Bristol 
    ##  3 Dan     Beasley-Harling      9        36 Full-time parent            London  
    ##  4 Imelda  McCarron             9        33 Countryside recreation off… County …
    ##  5 Jon     Jenkins              9        47 Blood courier               Newport 
    ##  6 Karen   Wright               9        60 In-store sampling assistant Wakefie…
    ##  7 Kim-Joy Hewlett              9        27 Mental health specialist    Leeds   
    ##  8 Luke    Thompson             9        30 Civil servant/house and te… Sheffie…
    ##  9 Manon   Lagrave              9        26 Software project manager    London  
    ## 10 Rahul   Mandal               9        30 Research scientist          Rotherh…
    ## # ℹ 15 more rows

``` r
missing_results
```

    ## # A tibble: 0 × 6
    ## # ℹ 6 variables: baker <chr>, baker_last_name <chr>, series <dbl>,
    ## #   baker_age <dbl>, baker_occupation <chr>, hometown <chr>

- **Description**: Description: We use `anti_join()` to identify any
  bakers present in the `bakes` or `results` datasets but missing from
  the `bakers` dataset. Addressing these discrepancies is crucial for a
  successful merge.

Step 3: Joing the Datasets

Once discrepancies are resolved, we join the three datasets to create a
comprehensive dataset for analysis.

``` r
# Join the bakes and results datasets
results_and_bakes <- 
  left_join(bakes, results, by = c("baker", "series", "episode"))

# Join with the bakers dataset
final_dataset <- 
  left_join(results_and_bakes, bakers, by = c("baker", "series")) |> 
  relocate(series, episode, baker, baker_last_name, baker_age, baker_occupation, hometown)

# Export the final dataset
write_csv(final_dataset, "data/final_bakeoff_dataset.csv")
```

- **Description**: We first join the `bakes` and `results` datasets by
  `baker`, `series`, and `episode`, then join this combined dataset with
  the `bakers` dataset. This gives us a complete dataset with baker
  details, performance data, and results for each episode.

- **Final Table Summary**: The final dataset serves as a complete,
  organized resource for analyzing the performance of bakers throughout
  the Great British Bake Off. With it, we can investigate trends like
  consistency in winning Star Baker, compare performance across series,
  and understand how individual bakers performed in different
  challenges. This dataset also allows for further exploratory analysis,
  such as predicting results based on performance metrics or identifying
  bakers with consistent success.

Step 4: Analysis of Winners (Seasons 5-10)

Next, we filter the results to focus on Star Bakers and Winners from
Seasons 5 through 10, and create a readable table.

``` r
#Filter for Star Bakers and Winners in Seasons 5-10
winners <- 
  results |> 
  filter(series <= 10, series >= 5) |> 
  filter(result %in% c("WINNER", "STAR BAKER")) |> 
  select(series, episode, baker, result)

#Create a table showing winners by episode for each series
winners |> 
  pivot_wider(
    names_from = series,   # Create columns for each series (5-10)
    values_from = baker    # Populate the columns with the baker names
  ) |> 
  arrange(episode) |>      # Arrange by episode
  knitr::kable()           # Create a nicely formatted table
```

| episode | result     | 5       | 6      | 7         | 8      | 9       | 10       |
|--------:|:-----------|:--------|:-------|:----------|:-------|:--------|:---------|
|       1 | STAR BAKER | Nancy   | Marie  | Jane      | Steven | Manon   | Michelle |
|       2 | STAR BAKER | Richard | Ian    | Candice   | Steven | Rahul   | Alice    |
|       3 | STAR BAKER | Luis    | Ian    | Tom       | Julia  | Rahul   | Michael  |
|       4 | STAR BAKER | Richard | Ian    | Benjamina | Kate   | Dan     | Steph    |
|       5 | STAR BAKER | Kate    | Nadiya | Candice   | Sophie | Kim-Joy | Steph    |
|       6 | STAR BAKER | Chetna  | Mat    | Tom       | Liam   | Briony  | Steph    |
|       7 | STAR BAKER | Richard | Tamal  | Andrew    | Steven | Kim-Joy | Henry    |
|       8 | STAR BAKER | Richard | Nadiya | Candice   | Stacey | Ruby    | Steph    |
|       9 | STAR BAKER | Richard | Nadiya | Andrew    | Sophie | Ruby    | Alice    |
|      10 | WINNER     | Nancy   | Nadiya | Candice   | Sophie | Rahul   | David    |

- **Description**: We filter the data to focus on Star Bakers and
  Winners from Seasons 5 to 10 and then use `pivot_wider()` to display
  the winners by episode for each series. The table is formatted using
  `knitr::kable()`.

- **Observation**:

- **Richard** from Season 5 stands out, winning the Star Baker title
  multiple times.

- **Nadiya** from Season 6 had a strong performance, particularly toward
  the later episodes.

- In Season 10, **Steph** won Star Baker in multiple episodes.

This information gives insight into the most consistent and
high-performing bakers over multiple episodes.

Step 5: Viewership Data

We now analyze the viewership data from the `viewers.csv` file.

``` r
# import data and display the first 10 rows of viewship data
viewers = read_csv('data/viewers.csv') |>
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
viewers |> 
  head(10) |> 
  knitr::kable()  # Optional: Use knitr::kable() for table formatting
```

| episode | series_1 | series_2 | series_3 | series_4 | series_5 | series_6 | series_7 | series_8 | series_9 | series_10 |
|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|
| 1 | 2.24 | 3.10 | 3.85 | 6.60 | 8.510 | 11.62 | 13.58 | 9.46 | 9.55 | 9.62 |
| 2 | 3.00 | 3.53 | 4.60 | 6.65 | 8.790 | 11.59 | 13.45 | 9.23 | 9.31 | 9.38 |
| 3 | 3.00 | 3.82 | 4.53 | 7.17 | 9.280 | 12.01 | 13.01 | 8.68 | 8.91 | 8.94 |
| 4 | 2.60 | 3.60 | 4.71 | 6.82 | 10.250 | 12.36 | 13.29 | 8.55 | 8.88 | 8.96 |
| 5 | 3.03 | 3.83 | 4.61 | 6.95 | 9.950 | 12.39 | 13.12 | 8.61 | 8.67 | 9.26 |
| 6 | 2.75 | 4.25 | 4.82 | 7.32 | 10.130 | 12.00 | 13.13 | 8.61 | 8.91 | 8.70 |
| 7 | NA | 4.42 | 5.10 | 7.76 | 10.280 | 12.35 | 13.45 | 9.01 | 9.22 | 8.98 |
| 8 | NA | 5.06 | 5.35 | 7.41 | 9.023 | 11.09 | 13.26 | 8.95 | 9.69 | 9.19 |
| 9 | NA | NA | 5.70 | 7.41 | 10.670 | 12.65 | 13.44 | 9.03 | 9.50 | 9.34 |
| 10 | NA | NA | 6.74 | 9.45 | 13.510 | 15.05 | 15.90 | 10.04 | 10.34 | 10.05 |

\-**Description**: This table shows the viewership (in millions) for
episodes across different series (1 to 10). The columns represent each
series, and the rows represent the episode number.

\-**Observation**:

- **Series 5 to 10** show generally higher viewership numbers compared
  to the earlier series (1 to 4), with viewership peaking in Series 7,
  Episode 10 at **15.90 million**.
- **Series 7 and Series 6** also had strong viewership, with several
  episodes drawing more than **13 million** viewers.

Step 6: Average Viewership for Series 1 and 5

- average viewership of series 1: **2.77**

- average viewership of series 5: **10.0393**
